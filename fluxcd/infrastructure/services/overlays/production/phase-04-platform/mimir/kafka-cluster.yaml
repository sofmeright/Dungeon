# KafkaNodePool - defines the node pool for Kafka brokers with dual roles (controller + broker)
# Using KRaft mode (ZooKeeper-less) with 3 replicas for quorum
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: mimir-kafka-pool
  namespace: gossip-stone
  labels:
    strimzi.io/cluster: mimir-kafka
spec:
  replicas: 3  # 3 brokers for 100 partitions @ RF=1 (~33 partitions per broker)
  # Dual-role nodes: both broker and controller (eliminates ZooKeeper dependency)
  roles:
    - controller
    - broker
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 100Gi
        deleteClaim: false
        class: ceph-rbd
        # KRaft metadata stored on each node
        kraftMetadata: shared
  resources:
    requests:
      cpu: 500m  # RF=1 eliminates replication overhead - much lighter workload
      memory: 2Gi  # Reduced from 3Gi - no inter-broker replication traffic
    limits:
      cpu: 2
      memory: 4Gi
---
# Kafka cluster resource - main configuration
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: mimir-kafka
  namespace: gossip-stone
  annotations:
    # Enable KRaft mode (ZooKeeper-less Kafka)
    strimzi.io/kraft: enabled
    # Enable KafkaNodePool usage
    strimzi.io/node-pools: enabled
spec:
  kafka:
    # Kafka version - using Strimzi 0.48.x supported version with KRaft support
    version: 4.1.0
    # Metadata version for KRaft compatibility
    metadataVersion: 4.1-IV0
    # Kafka image - use JCR pull-through cache for bandwidth efficiency
    image: quay.jcr.pcfae.com/strimzi/kafka:0.48.0-kafka-4.1.0
    # Listeners for client connections
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    # Kafka configuration
    config:
      # Replication settings - RF=1 because Ceph provides 4x replication underneath
      # Effective replication: 1 (Kafka) × 4 (Ceph) = 4 copies (vs previous 3×4=12)
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
      # Default replication for new topics
      default.replication.factor: 1
      min.insync.replicas: 1
      # Log retention for Mimir ingest storage (48h industry standard - buffer for ingester failures)
      log.retention.hours: 48
      # Log segment size
      log.segment.bytes: 1073741824  # 1GB segments
      # Log cleanup
      log.cleanup.policy: delete
      log.retention.check.interval.ms: 300000  # Check every 5 minutes
      # Message size limits (10MB to handle high-cardinality metrics)
      message.max.bytes: 10485760  # 10MB - max message size from producer
      replica.fetch.max.bytes: 10485760  # 10MB - allow replication of large messages
      # Compression
      compression.type: producer
      # Auto-create topics (Mimir will create its own topics)
      auto.create.topics.enable: true
    # Metrics configuration
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          name: mimir-kafka-metrics
          key: kafka-metrics-config.yaml
  # Entity Operator manages topics and users
  entityOperator:
    topicOperator:
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi
    userOperator:
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi
  # Kafka Exporter for metrics
  kafkaExporter:
    topicRegex: ".*"
    groupRegex: ".*"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
