apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: mimir
spec:
  values:
    # Production-specific storage class configuration - using static CephFS PVCs
    ingester:
      persistentVolume:
        storageClass: ""  # Use existing static PVCs (CephFS)
      # Mount S3 credentials for block uploads
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3
      # Pod anti-affinity to spread across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/component
                      operator: In
                      values:
                        - ingester
                topologyKey: kubernetes.io/hostname
      # Resource limits - increased memory for CephFS write performance
      resources:
        requests:
          cpu: 200m
          memory: 2Gi
        limits:
          cpu: 1
          memory: 4Gi

    store_gateway:
      persistentVolume:
        storageClass: ""  # Use existing static PVCs (CephFS)
      # Mount S3 credentials for block queries
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3

    compactor:
      persistentVolume:
        storageClass: ""  # Use existing static PVCs (CephFS)
      # Mount S3 credentials for block compaction
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3

    querier:
      # Mount S3 credentials for querying blocks from storage
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3

    # Global imagePullSecrets for all Mimir pods
    global:
      imagePullSecrets:
        - name: jcr-pcfae-dungeon-pull-secret

    # Image configuration - use JCR pull-through cache for bandwidth efficiency
    # CRI-O requires fully qualified image names in short-name enforcement mode
    image:
      repository: docker.jcr.pcfae.com/grafana/mimir
      # tag: use chart default (inherits from chart version)

    rollout_operator:
      image:
        repository: docker.jcr.pcfae.com/grafana/rollout-operator
        # tag: use chart default

    rolloutOperator:
      image:
        repository: docker.jcr.pcfae.com/grafana/rollout-operator
        # tag: use chart default

    gateway:
      nginx:
        image:
          registry: docker.jcr.pcfae.com
          repository: nginxinc/nginx-unprivileged
          # tag: use chart default

    serviceMonitor:
      enabled: false  # Using Alloy service discovery instead

    # Distributor resource limits - increased for high-cardinality metrics
    # TEMPORARY: 4Gi limit to handle metric backlog from power outage recovery
    # TODO: Reduce back to 2Gi after backlog is processed
    distributor:
      resources:
        requests:
          cpu: 200m
          memory: 1Gi
        limits:
          cpu: 1
          memory: 4Gi

    # Ingest Storage DISABLED - using classic mode with direct S3 writes
    ingest_storage:
      enabled: false

    # Kafka DISABLED - not needed for classic mode
    kafka:
      enabled: false

    mimir:
      structuredConfig:
        # Explicitly disable ingest storage in structured config
        ingest_storage:
          enabled: false

        distributor:
          ring:
            kvstore:
              store: memberlist

        # S3 blocks storage configuration - using Ceph RGW
        common:
          storage:
            backend: s3
            s3:
              endpoint: ceph-rgw.gorons-bracelet.svc.cluster.local:80
              region: default
              bucket_name: gossip-stone-mimir
              access_key_id: ${s3_access_key}
              secret_access_key: ${s3_secret_key}
              insecure: true

        blocks_storage:
          backend: s3
          s3:
            endpoint: ceph-rgw.gorons-bracelet.svc.cluster.local:80
            region: default
            bucket_name: gossip-stone-mimir
            access_key_id: ${s3_access_key}
            secret_access_key: ${s3_secret_key}
            insecure: true
          tsdb:
            dir: /data/tsdb
          bucket_store:
            sync_dir: /data/tsdb-sync

        ingester:
          push_grpc_method_enabled: true  # Required when ingest_storage is disabled
          ring:
            kvstore:
              store: memberlist
            replication_factor: 3
            zone_awareness_enabled: false

        store_gateway:
          sharding_ring:
            kvstore:
              store: memberlist
            replication_factor: 1

        compactor:
          sharding_ring:
            kvstore:
              store: memberlist

        memberlist:
          join_members:
            - dns+mimir-gossip-ring.gossip-stone.svc.cluster.local:7946

        alertmanager:
          data_dir: /data/alertmanager

        alertmanager_storage:
          backend: filesystem
          filesystem:
            dir: /data/blocks

        limits:
          max_global_series_per_user: 0
          ingestion_rate: 200000
          ingestion_burst_size: 400000
          max_query_parallelism: 32
          max_label_names_per_series: 50  # Increased from default 30 to support Docker Compose labels
          out_of_order_time_window: 2h  # Allow out-of-order samples to recover from power outage S3 timestamp skew
