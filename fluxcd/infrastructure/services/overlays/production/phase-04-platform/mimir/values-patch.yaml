apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: mimir
spec:
  values:
    # Production-specific storage class configuration - using RBD for TSDB performance
    ingester:
      persistentVolume:
        storageClass: ceph-rbd  # Fast block storage for TSDB writes
        size: 20Gi  # Provides ~85 hours (3.5 days) buffer with aggressive compaction - enough for long weekends
      # Mount S3 credentials for block uploads
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3
      # Pod anti-affinity to spread across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/component
                      operator: In
                      values:
                        - ingester
                topologyKey: kubernetes.io/hostname
      # Resource limits for ingestion
      # Request = typical operation (~500m CPU, ~4Gi mem)
      # Limit = headroom for compaction/recovery spikes (observed 3Gi CPU, 8Gi mem)
      resources:
        requests:
          cpu: 500m
          memory: 4Gi
        limits:
          cpu: 4
          memory: 10Gi

    store_gateway:
      persistentVolume:
        storageClass: ceph-rbd  # Fast block storage for block index cache
        size: 15Gi  # Block index cache and metadata
      # Mount S3 credentials for block queries
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3

    compactor:
      persistentVolume:
        storageClass: ceph-rbd  # Fast block storage for compaction work
        size: 25Gi  # Compaction working directory - needs extra space for concurrent block merging
      # Mount S3 credentials for block compaction
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3
      resources:
        requests:
          cpu: 200m
          memory: 2Gi
        limits:
          cpu: 2
          memory: 4Gi

    querier:
      # Mount S3 credentials for querying blocks from storage
      extraArgs:
        -config.expand-env: true
      extraEnvFrom:
        - secretRef:
            name: mimir-s3

    # Global imagePullSecrets for all Mimir pods
    global:
      imagePullSecrets:
        - name: jcr-pcfae-dungeon-pull-secret

    # Image configuration - use JCR pull-through cache for bandwidth efficiency
    # CRI-O requires fully qualified image names in short-name enforcement mode
    image:
      repository: docker.jcr.pcfae.com/grafana/mimir
      # tag: use chart default (inherits from chart version)

    rollout_operator:
      image:
        repository: docker.jcr.pcfae.com/grafana/rollout-operator
        # tag: use chart default

    rolloutOperator:
      image:
        repository: docker.jcr.pcfae.com/grafana/rollout-operator
        # tag: use chart default

    gateway:
      image:
        repository: docker.jcr.pcfae.com/nginxinc/nginx-unprivileged
        # tag: use chart default

    serviceMonitor:
      enabled: false  # Using Alloy service discovery instead

    # Distributor resource limits - handles metrics ingestion and routing
    # Memory increased to 8Gi - OOMKilled at 4Gi and 6Gi under heavy Alloy load
    distributor:
      resources:
        requests:
          cpu: 200m
          memory: 4Gi
        limits:
          cpu: 2
          memory: 8Gi

    # Ingest Storage DISABLED - using classic mode with direct S3 writes
    ingest_storage:
      enabled: false

    # Kafka DISABLED - not needed for classic mode
    kafka:
      enabled: false

    mimir:
      structuredConfig:
        # Explicitly disable ingest storage in structured config
        ingest_storage:
          enabled: false

        distributor:
          remote_timeout: 10s  # Increased from default 2s to tolerate slower writes under load
          ring:
            kvstore:
              store: memberlist

        # S3 blocks storage configuration - using Ceph RGW
        common:
          storage:
            backend: s3
            s3:
              endpoint: ceph-rgw.gorons-bracelet.svc.cluster.local:80
              region: default
              bucket_name: gossip-stone-mimir
              access_key_id: ${s3_access_key}
              secret_access_key: ${s3_secret_key}
              insecure: true

        blocks_storage:
          backend: s3
          s3:
            endpoint: ceph-rgw.gorons-bracelet.svc.cluster.local:80
            region: default
            bucket_name: gossip-stone-mimir
            access_key_id: ${s3_access_key}
            secret_access_key: ${s3_secret_key}
            insecure: true
          tsdb:
            dir: /data/tsdb
            # WAL settings - compress WAL to reduce disk usage
            wal_compression_enabled: true  # Compress WAL to reduce disk usage
            wal_segment_size_bytes: 134217728  # 128MB WAL segments (default: 256MB) - smaller segments = more frequent flushing
            # Block range for compaction - use 2-hour blocks matching out-of-order window
            block_ranges_period: ["2h"]  # 2 hours - must match out_of_order_time_window
            retention_period: 6h  # Only keep blocks in ingester for 6 hours before upload to S3 is mandatory
          bucket_store:
            sync_dir: /data/tsdb-sync
            # Increase sync interval to reduce load
            sync_interval: 15m  # Default: 15m

        ingester:
          push_grpc_method_enabled: true  # Required when ingest_storage is disabled
          ring:
            kvstore:
              store: memberlist
            replication_factor: 3
            zone_awareness_enabled: false
          # Limit in-memory series to prevent OOM and encourage flushing to disk/S3
          instance_limits:
            max_series: 1500000  # Max series per ingester instance
            max_ingestion_rate: 200000  # Max samples/sec per ingester (increased from 80k for initial data load)
          # Close idle TSDB heads faster to free up disk space
          tsdb_config_update_period: 15s  # Check for config updates every 15s

        store_gateway:
          sharding_ring:
            kvstore:
              store: memberlist
            replication_factor: 1

        compactor:
          sharding_ring:
            kvstore:
              store: memberlist
          # Compaction settings to handle corrupted blocks and reduce disk pressure
          compaction_interval: 30m  # Run compaction every 30 minutes
          data_dir: /data/compactor  # Working directory for compaction
          cleanup_interval: 15m  # Clean up old blocks every 15 minutes
          deletion_delay: 2h  # Wait 2 hours before deleting compacted blocks
          max_compaction_time: 4h  # Allow up to 4 hours for compaction jobs
          # Skip corrupted blocks instead of failing entire compaction
          block_sync_concurrency: 8  # Download up to 8 blocks concurrently

        memberlist:
          join_members:
            - dns+mimir-gossip-ring.gossip-stone.svc.cluster.local:7946

        alertmanager:
          data_dir: /data/alertmanager

        alertmanager_storage:
          backend: filesystem
          filesystem:
            dir: /data/blocks

        limits:
          max_global_series_per_user: 0  # Unlimited series (use ingester instance_limits instead)
          # REDUCED from 200k/400k - previous rate caused 10Gi disks to fill in 18 hours
          # New rate (120k/240k total with 3 ingesters) balances performance with 50Gi disk capacity (~7 days at current load)
          ingestion_rate: 500000  # Max samples/sec per user across all distributors (increased for initial load)
          ingestion_burst_size: 1000000  # Burst capacity for traffic spikes (increased)
          max_query_parallelism: 32  # Max concurrent query operations
          max_label_names_per_series: 50  # Increased from default 30 to support Docker Compose labels
          # Out-of-order window - allows acceptance of late-arriving samples
          # Reduced to 1h - 6h caused ingesters to OOM (needed 18Gi+ memory)
          out_of_order_time_window: 1h
